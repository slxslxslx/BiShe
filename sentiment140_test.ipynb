{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtiRAv4vXndjl8ue5ecpxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slxslxslx/BiShe/blob/main/sentiment140_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mqp6q5mLoDZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hTS0c9Vn3sC",
        "outputId": "ee97b477-8977-4607-f336-c4796d8efb2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N3-1pTlLn-yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 尝试-这个分布式可以顺利运行\n",
        "# ds = load_dataset(\"contemmcm/sentiment140\")\n",
        "# 百万级别1600000，二分类\n",
        "# 训练集数据量: DatasetDict({\n",
        "#     complete: Dataset({\n",
        "#         features: ['text', 'label'],\n",
        "#         num_rows: 1600000\n",
        "#     })\n",
        "# })\n",
        "# RTX 2080 Ti * 2\n",
        "# Single GPU training time: 212.33 seconds\n",
        "#    accuracy                           0.88\n",
        "# 多GPU,rank=1 end_timeMulti GPU training time: 149.43 seconds\n",
        "#     accuracy                           0.88\n",
        "import os\n",
        "\n",
        "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split, DistributedSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# 获取北京时区对象\n",
        "beijing_tz = timezone(timedelta(hours=8))\n",
        "\n",
        "\n",
        "# 获取当前的UTC时间并转换为北京时间\n",
        "def get_beijing_time_now():\n",
        "    return datetime.now(beijing_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "# 加载IMDB数据集和BERT分词器，并进行一些初始化操作\n",
        "def load_and_preprocess_data():\n",
        "    # print('load_and_preprocess_data开始执行')\n",
        "    # dataset = load_dataset(\"contemmcm/sentiment140\", cache_dir=\"./dataset/sentiment140\")\n",
        "    # print(\"dataset['complete'][:5]:\", dataset['complete'][:5])\n",
        "    # tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # # 文本编码函数\n",
        "    # def encode_batch(batch):\n",
        "    #     return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # # 编码数据集\n",
        "    # print('开始编码数据集',get_beijing_time_now())\n",
        "    # encoded_dataset = dataset.map(encode_batch, batched=True)\n",
        "    # encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    # print(\"encoded_dataset['complete'][:5]:\", encoded_dataset['complete'][:5])\n",
        "    # print('结束编码数据集',get_beijing_time_now())\n",
        "\n",
        "    # # 保存encoded_dataset\n",
        "    # with open('encoded_dataset-sentiment140.pkl','wb') as f:\n",
        "    #     pickle.dump(encoded_dataset, f)\n",
        "    #     print('保存encoded_dataset-sentiment140.pkl')\n",
        "\n",
        "    with open('encoded_dataset-sentiment140.pkl', 'rb') as f:\n",
        "        encoded_dataset = pickle.load(f)\n",
        "\n",
        "    # 划分训练集和测试集\n",
        "    # 假设encoded_dataset['complete']是你的数据集，这里获取其长度\n",
        "    total_length = len(encoded_dataset['complete'])\n",
        "    # 计算训练集的大小（这里按80%比例计算，可根据实际需求修改比例）\n",
        "    train_size = int(total_length * 0.8)\n",
        "    # 计算测试集的大小\n",
        "    test_size = total_length - train_size\n",
        "    print('total_length', total_length)\n",
        "    print('train_size', train_size)\n",
        "    print('test_size', test_size)\n",
        "    # train_dataset, test_dataset = random_split(encoded_dataset['complete'], [20000, 5000])\n",
        "    train_dataset, test_dataset = random_split(encoded_dataset['complete'], [train_size, test_size])\n",
        "\n",
        "    # 使用数据集中自带的train部分作为训练集\n",
        "    # train_dataset = encoded_dataset['train']\n",
        "    # test_dataset = encoded_dataset['test']\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "    print('load_and_preprocess_data执行完毕')\n",
        "    return train_dataset, test_dataset, test_loader\n",
        "\n",
        "\n",
        "# 单GPU训练\n",
        "def train_single_gpu(model, train_dataset, epochs=1):\n",
        "    print('单GPU训练开始,epochs=', epochs)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-5)  # 优化器用于根据计算得到的损失值来更新模型的参数\n",
        "    # 以使得模型能够朝着损失值不断减小的方向进行优化，进而提高模型对数据的拟合能力和预测准确性。\n",
        "    # 自适应矩估计（Adaptive Moment Estimation）优化算法\n",
        "    # model.parameters() 获取模型中所有需要被优化的参数\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('单GPU start_time', get_beijing_time_now())\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch', epoch)\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('单GPU end_time', get_beijing_time_now())\n",
        "    print(f\"Single GPU training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # 评估模型\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():  # 不进行梯度计算\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print('单GPU评估:\\n', classification_report(all_labels, all_preds))\n",
        "\n",
        "\n",
        "# 2GPU训练\n",
        "def train_multi_gpu(rank, world_size, model, train_dataset, test_loader, epochs=1):\n",
        "    # print('test_loader', test_loader)\n",
        "    print(f\"多GPU分布式训练进程 {rank} 初始化\")\n",
        "\n",
        "    # 设置分布式环境变量\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size, timeout=timedelta(seconds=180))\n",
        "\n",
        "    # 设定设备\n",
        "    device = torch.device(f\"cuda:{rank}\")\n",
        "    model = model.to(device)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "    # device_ids=[rank] 这个参数就是明确告诉 DDP 模块，当前这个模型副本应该放置在哪个具体的 GPU 设备上进行后续的训练操作。保证每个进程所负责的模型副本和相应的 GPU 紧密关联起来\n",
        "\n",
        "    # 使用 DistributedSampler 确保每个进程处理不同的数据\n",
        "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, sampler=train_sampler)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(f'多GPU,rank={rank} start_time', get_beijing_time_now())\n",
        "    if rank == 0:\n",
        "        print(f\"多GPU训练开始时间: {get_beijing_time_now()}\")\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if rank == 0:\n",
        "            print(f\"Epoch {epoch} 开始\")\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for batch in train_loader:\n",
        "            # print(rank, 'batch')\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f'多GPU,rank={rank} end_time', get_beijing_time_now())\n",
        "    if rank == 0:\n",
        "        print(f\"Multi GPU training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # 评估模型\n",
        "    if rank == 0:\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print('多GPU评估:\\n', classification_report(all_labels, all_preds))\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "# 设置进程启动，通过mp.spawn启动多进程分布式训练\n",
        "def spawn_train(world_size, model, train_dataset, test_loader):\n",
        "    mp.spawn(train_multi_gpu, args=(world_size, model, train_dataset, test_loader), nprocs=world_size,\n",
        "             join=True)  # join=True时mp.spawn 函数会阻塞主进程，直到所有启动的子进程都执行完毕。\n",
        "\n",
        "\n",
        "# 主函数入口\n",
        "if __name__ == '__main__':\n",
        "    train_dataset, test_dataset, test_loader = load_and_preprocess_data()\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "    criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数\n",
        "\n",
        "    # 单GPU训练\n",
        "    train_single_gpu(model, train_dataset)\n",
        "\n",
        "    # 多GPU分布式训练!\n",
        "    # spawn_train(world_size=2, model=model, train_dataset=train_dataset, test_loader=test_loader)\n",
        "\n",
        "    print('main')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "UEzfvwcJn7Jm",
        "outputId": "53ea0c1e-4b9d-4a99-f0ac-9045e9022973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Failed to open local file '/content/dataset/sentiment140/contemmcm___sentiment140/default/0.0.0/3fbbc6d5cab2192c8cb7591bdf5b1d4360ea74fd/cache-7ee2a0e947c4ccf8.arrow'. Detail: [errno 2] No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-41f389c7c170>\u001b[0m in \u001b[0;36m<cell line: 218>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;31m# 主函数入口\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 交叉熵损失函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-41f389c7c170>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoded_dataset-sentiment140.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mencoded_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# 划分训练集和测试集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mreplays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"replays\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_memory_mapped_arrow_table_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_replays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0mMemoryMappedTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m_memory_mapped_arrow_table_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_memory_mapped_arrow_table_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mopened_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_memory_mapped_record_batch_reader_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m_memory_mapped_record_batch_reader_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_memory_mapped_record_batch_reader_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecordBatchStreamReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmemory_mapped_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_mapped_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.memory_map\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '/content/dataset/sentiment140/contemmcm___sentiment140/default/0.0.0/3fbbc6d5cab2192c8cb7591bdf5b1d4360ea74fd/cache-7ee2a0e947c4ccf8.arrow'. Detail: [errno 2] No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIpcxebErrda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqO3o1_mrrle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOG_BQkXrrsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZcMug-qrryg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEbr70Pxrr4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split, DistributedSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.cuda.amp import GradScaler, autocast  # 【新增】支持混合精度训练\n",
        "from torch.utils.tensorboard import SummaryWriter  # 【新增】支持日志记录\n",
        "\n",
        "import pickle\n",
        "\n",
        "# 获取北京时区对象\n",
        "beijing_tz = timezone(timedelta(hours=8))\n",
        "\n",
        "\n",
        "# 获取当前的UTC时间并转换为北京时间\n",
        "def get_beijing_time_now():\n",
        "    return datetime.now(beijing_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "# 加载IMDB数据集和BERT分词器，并进行一些初始化操作\n",
        "def load_and_preprocess_data():\n",
        "    print('load_and_preprocess_data开始执行')\n",
        "    dataset = load_dataset(\"contemmcm/sentiment140\", cache_dir=\"./dataset/sentiment140\")\n",
        "    print(\"dataset['complete'][:5]:\", dataset['complete'][:5])\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # # 文本编码函数\n",
        "    # def encode_batch(batch):\n",
        "    #     return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # # 编码数据集\n",
        "    # print('开始编码数据集', get_beijing_time_now())\n",
        "    # encoded_dataset = dataset.map(encode_batch, batched=True)\n",
        "    # encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    # print(\"encoded_dataset['complete'][:5]:\", encoded_dataset['complete'][:5])\n",
        "    # print('结束编码数据集', get_beijing_time_now())\n",
        "\n",
        "    with open('encoded_dataset-sentiment140.pkl', 'rb') as f:\n",
        "        encoded_dataset = pickle.load(f)\n",
        "        print('加载encoded_dataset-sentiment140.pkl完毕')\n",
        "\n",
        "    # 划分训练集和测试集\n",
        "    total_length = len(encoded_dataset['complete'])\n",
        "    train_size = int(total_length * 0.8)\n",
        "    test_size = total_length - train_size\n",
        "    train_dataset, test_dataset = random_split(encoded_dataset['complete'], [train_size, test_size])\n",
        "\n",
        "    print('total_length', total_length)\n",
        "    print('train_size', train_size)\n",
        "    print('test_size', test_size)\n",
        "\n",
        "    print('load_and_preprocess_data执行完毕')\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "# 单GPU训练\n",
        "def train_single_gpu(model, train_dataset, test_dataset, epochs=3):\n",
        "    print('单GPU训练开始,epochs=', epochs)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    batch_size = 64  # 【修改】适配更大的 batch_size，充分利用显存\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)  # 【修改】调高学习率\n",
        "    criterion = nn.CrossEntropyLoss()  # 损失函数\n",
        "    scaler = GradScaler()  # 【新增】混合精度训练支持\n",
        "    writer = SummaryWriter(log_dir='./logs')  # 【新增】日志记录\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('单GPU start_time', get_beijing_time_now())\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch', epoch)\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():  # 【新增】混合精度上下文\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch}: Loss = {epoch_loss / len(train_loader):.4f}\")\n",
        "        writer.add_scalar(\"Loss/train\", epoch_loss / len(train_loader), epoch)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('单GPU end_time', get_beijing_time_now())\n",
        "    print(f\"Single GPU training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # 评估模型\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print('单GPU评估:\\n', classification_report(all_labels, all_preds))\n",
        "\n",
        "\n",
        "# 2GPU分布式训练\n",
        "def train_multi_gpu(rank, world_size, model, train_dataset, test_dataset, epochs=3):\n",
        "    print(f\"多GPU分布式训练进程 {rank} 初始化\")\n",
        "\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size, timeout=timedelta(seconds=180))\n",
        "\n",
        "    device = torch.device(f\"cuda:{rank}\")\n",
        "    model = model.to(device)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "\n",
        "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler, num_workers=4, pin_memory=True)\n",
        "\n",
        "    test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, sampler=test_sampler, num_workers=4, pin_memory=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "    end_time = time.time()\n",
        "    if rank == 0:\n",
        "        print(f\"Multi GPU training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "# 启动分布式训练\n",
        "def spawn_train(world_size, model, train_dataset, test_dataset):\n",
        "    mp.spawn(train_multi_gpu, args=(world_size, model, train_dataset, test_dataset), nprocs=world_size, join=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dataset, test_dataset = load_and_preprocess_data()\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "    # 单GPU训练\n",
        "    train_single_gpu(model, train_dataset, test_dataset)\n",
        "\n",
        "    # 多GPU训练\n",
        "    # spawn_train(world_size=2, model=model, train_dataset=train_dataset, test_dataset=test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "l_nzDxm9rblo",
        "outputId": "e27f5085-4319-4590-a6e3-631d947f48a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_and_preprocess_data开始执行\n",
            "dataset['complete'][:5]: {'text': [\"Just back home from a little gathering with some old friends.. It was really fun, they're still the same. \", 'Hey @ricebunny i need a web cam!!!   (RiceBunny live > http://ustre.am/ZbT)', 'only a couple more days before i have to go to faggot summer school ', 'Must admit Zac handled it well!!! ', \"It's warm and sticky but there's no sun  *Sneeze*\"], 'label': [1, 0, 0, 0, 0]}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Failed to open local file '/content/dataset/sentiment140/contemmcm___sentiment140/default/0.0.0/3fbbc6d5cab2192c8cb7591bdf5b1d4360ea74fd/cache-7ee2a0e947c4ccf8.arrow'. Detail: [errno 2] No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3dfc122a505e>\u001b[0m in \u001b[0;36m<cell line: 181>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3dfc122a505e>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoded_dataset-sentiment140.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mencoded_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'加载encoded_dataset-sentiment140.pkl完毕'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mreplays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"replays\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_memory_mapped_arrow_table_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_replays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0mMemoryMappedTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m_memory_mapped_arrow_table_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_memory_mapped_arrow_table_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mopened_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_memory_mapped_record_batch_reader_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m_memory_mapped_record_batch_reader_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_memory_mapped_record_batch_reader_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecordBatchStreamReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmemory_mapped_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_mapped_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.memory_map\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '/content/dataset/sentiment140/contemmcm___sentiment140/default/0.0.0/3fbbc6d5cab2192c8cb7591bdf5b1d4360ea74fd/cache-7ee2a0e947c4ccf8.arrow'. Detail: [errno 2] No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 尝试-这个分布式可以顺利运行\n",
        "# ds = load_dataset(\"contemmcm/sentiment140\")\n",
        "# 百万级别1600000，二分类\n",
        "# 训练集数据量: DatasetDict({\n",
        "#     complete: Dataset({\n",
        "#         features: ['text', 'label'],\n",
        "#         num_rows: 1600000\n",
        "#     })\n",
        "# })\n",
        "# RTX 2080 Ti * 2\n",
        "# Single GPU training time: 212.33 seconds\n",
        "#    accuracy                           0.88\n",
        "# 多GPU,rank=1 end_timeMulti GPU training time: 149.43 seconds\n",
        "#     accuracy                           0.88\n",
        "import os\n",
        "\n",
        "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "import time\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split, DistributedSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# 获取北京时区对象\n",
        "beijing_tz = timezone(timedelta(hours=8))\n",
        "\n",
        "\n",
        "# 获取当前的UTC时间并转换为北京时间\n",
        "def get_beijing_time_now():\n",
        "    return datetime.now(beijing_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "\n",
        "# 加载IMDB数据集和BERT分词器，并进行一些初始化操作\n",
        "def load_and_preprocess_data():\n",
        "    # print('load_and_preprocess_data开始执行')\n",
        "    # dataset = load_dataset(\"contemmcm/sentiment140\", cache_dir=\"./dataset/sentiment140\")\n",
        "    # print(\"dataset['complete'][:5]:\", dataset['complete'][:5])\n",
        "    # tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # # 文本编码函数\n",
        "    # def encode_batch(batch):\n",
        "    #     return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # # 编码数据集\n",
        "    # print('开始编码数据集',get_beijing_time_now())\n",
        "    # encoded_dataset = dataset.map(encode_batch, batched=True)\n",
        "    # encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    # print(\"encoded_dataset['complete'][:5]:\", encoded_dataset['complete'][:5])\n",
        "    # print('结束编码数据集',get_beijing_time_now())\n",
        "\n",
        "    # # 保存encoded_dataset\n",
        "    # with open('encoded_dataset-sentiment140.pkl','wb') as f:\n",
        "    #     pickle.dump(encoded_dataset, f)\n",
        "    #     print('保存encoded_dataset-sentiment140.pkl')\n",
        "\n",
        "    with open('encoded_dataset-sentiment140.pkl', 'rb') as f:\n",
        "        encoded_dataset = pickle.load(f)\n",
        "\n",
        "    # 划分训练集和测试集\n",
        "    # 假设encoded_dataset['complete']是你的数据集，这里获取其长度\n",
        "    total_length = len(encoded_dataset['complete'])\n",
        "    # 计算训练集的大小（这里按80%比例计算，可根据实际需求修改比例）\n",
        "    train_size = int(total_length * 0.8)\n",
        "    # 计算测试集的大小\n",
        "    test_size = total_length - train_size\n",
        "    print('total_length', total_length)\n",
        "    print('train_size', train_size)\n",
        "    print('test_size', test_size)\n",
        "    # train_dataset, test_dataset = random_split(encoded_dataset['complete'], [20000, 5000])\n",
        "    train_dataset, test_dataset = random_split(encoded_dataset['complete'], [train_size, test_size])\n",
        "\n",
        "    # 使用数据集中自带的train部分作为训练集\n",
        "    # train_dataset = encoded_dataset['train']\n",
        "    # test_dataset = encoded_dataset['test']\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "    print('load_and_preprocess_data执行完毕')\n",
        "    return train_dataset, test_dataset, test_loader\n",
        "\n",
        "\n",
        "# 单GPU训练\n",
        "def train_single_gpu(model, train_dataset, epochs=1):\n",
        "    print('单GPU训练开始,epochs=', epochs)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-5)  # 优化器用于根据计算得到的损失值来更新模型的参数\n",
        "    # 以使得模型能够朝着损失值不断减小的方向进行优化，进而提高模型对数据的拟合能力和预测准确性。\n",
        "    # 自适应矩估计（Adaptive Moment Estimation）优化算法\n",
        "    # model.parameters() 获取模型中所有需要被优化的参数\n",
        "\n",
        "    start_time = time.time()\n",
        "    print('单GPU start_time', get_beijing_time_now())\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch', epoch)\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('单GPU end_time', get_beijing_time_now())\n",
        "    print(f\"Single GPU training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # 评估模型\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():  # 不进行梯度计算\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print('单GPU评估:\\n', classification_report(all_labels, all_preds))\n",
        "\n",
        "\n",
        "# 2GPU训练\n",
        "def train_multi_gpu(rank, world_size, model, train_dataset, test_loader, epochs=1):\n",
        "    # print('test_loader', test_loader)\n",
        "    print(f\"多GPU分布式训练进程 {rank} 初始化\")\n",
        "\n",
        "    # 设置分布式环境变量\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size, timeout=timedelta(seconds=180))\n",
        "\n",
        "    # 设定设备\n",
        "    device = torch.device(f\"cuda:{rank}\")\n",
        "    model = model.to(device)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "    # device_ids=[rank] 这个参数就是明确告诉 DDP 模块，当前这个模型副本应该放置在哪个具体的 GPU 设备上进行后续的训练操作。保证每个进程所负责的模型副本和相应的 GPU 紧密关联起来\n",
        "\n",
        "    # 使用 DistributedSampler 确保每个进程处理不同的数据\n",
        "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, sampler=train_sampler)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(f'多GPU,rank={rank} start_time', get_beijing_time_now())\n",
        "    if rank == 0:\n",
        "        print(f\"多GPU训练开始时间: {get_beijing_time_now()}\")\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if rank == 0:\n",
        "            print(f\"Epoch {epoch} 开始\")\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for batch in train_loader:\n",
        "            # print(rank, 'batch')\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f'多GPU,rank={rank} end_time', get_beijing_time_now())\n",
        "    if rank == 0:\n",
        "        print(f\"Multi GPU training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # 评估模型\n",
        "    if rank == 0:\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print('多GPU评估:\\n', classification_report(all_labels, all_preds))\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "# 设置进程启动，通过mp.spawn启动多进程分布式训练\n",
        "def spawn_train(world_size, model, train_dataset, test_loader):\n",
        "    mp.spawn(train_multi_gpu, args=(world_size, model, train_dataset, test_loader), nprocs=world_size,\n",
        "             join=True)  # join=True时mp.spawn 函数会阻塞主进程，直到所有启动的子进程都执行完毕。\n",
        "\n",
        "\n",
        "# 主函数入口\n",
        "if __name__ == '__main__':\n",
        "    train_dataset, test_dataset, test_loader = load_and_preprocess_data()\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "    criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数\n",
        "\n",
        "    # 单GPU训练\n",
        "    train_single_gpu(model, train_dataset)\n",
        "\n",
        "    # 多GPU分布式训练!\n",
        "    # spawn_train(world_size=2, model=model, train_dataset=train_dataset, test_loader=test_loader)\n",
        "\n",
        "    print('main')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "umwJc-ZixbS6",
        "outputId": "adce2657-9278-4e26-da86-38db21413fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Failed to open local file '/content/dataset/sentiment140/contemmcm___sentiment140/default/0.0.0/3fbbc6d5cab2192c8cb7591bdf5b1d4360ea74fd/cache-7ee2a0e947c4ccf8.arrow'. Detail: [errno 2] No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-41f389c7c170>\u001b[0m in \u001b[0;36m<cell line: 218>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;31m# 主函数入口\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 交叉熵损失函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-41f389c7c170>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoded_dataset-sentiment140.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mencoded_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# 划分训练集和测试集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mreplays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"replays\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_memory_mapped_arrow_table_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_replays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0mMemoryMappedTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m_memory_mapped_arrow_table_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_memory_mapped_arrow_table_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mopened_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_memory_mapped_record_batch_reader_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36m_memory_mapped_record_batch_reader_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_memory_mapped_record_batch_reader_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecordBatchStreamReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmemory_mapped_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_mapped_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.memory_map\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '/content/dataset/sentiment140/contemmcm___sentiment140/default/0.0.0/3fbbc6d5cab2192c8cb7591bdf5b1d4360ea74fd/cache-7ee2a0e947c4ccf8.arrow'. Detail: [errno 2] No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chmod +w /content/dataset/sentiment140"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "b9xd9NHpxf-6",
        "outputId": "c2cb4557-8cd9-41f1-b311-6fe5b65ab725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chmod' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ac88d3a17383>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchmod\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msentiment140\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'chmod' is not defined"
          ]
        }
      ]
    }
  ]
}